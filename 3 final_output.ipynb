{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_clean_df.py\n",
    "\n",
    "cleans the data to be read into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/create_clean_df.py\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_json('data/data.zip')\n",
    "    df['fraud'] = df['acct_type'].apply(lambda x: 'fraud' in x)\n",
    "    df = df[df['acct_type']!='spammer']\n",
    "    df.to_pickle('data/labelled_dataframe.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py\n",
    "\n",
    "creates a model based on the company training data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class MyModel():\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier(n_estimators=1000, max_features=9)\n",
    "        self.features = [\"body_length\",\"channels\",\"delivery_method\",\"fb_published\",\"gts\",\"org_facebook\",\"org_twitter\",\"user_age\",\"has_header\",\"venue_longitude\",\"payout_type_num\",\"user_type\"]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Map payout type to integers\n",
    "        X[\"payout_type_num\"] = X[\"payout_type\"].map({\"ACH\":1,\"CHECK\":2})\n",
    "        X[\"payout_type_num\"].fillna(3,inplace = True)\n",
    "        # Get only the predictive features\n",
    "        X = X[self.features]\n",
    "        # Fill missing value headers to False\n",
    "        X[\"has_header\"].fillna(0,inplace=True)\n",
    "        # Fill remaining few missing with median\n",
    "        self.median = X.median()\n",
    "        X.fillna(self.median,inplace= True)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Map payout type to integers\n",
    "        X[\"payout_type_num\"] = X[\"payout_type\"].map({\"ACH\":1,\"CHECK\":2})\n",
    "        X[\"payout_type_num\"].fillna(3,inplace = True)\n",
    "        # Get only the predictive features\n",
    "        X = X[self.features]\n",
    "        # Fill missing value headers to False\n",
    "        X[\"has_header\"].fillna(0,inplace=True)\n",
    "        # Fill remaining few missing with median\n",
    "        X.fillna(self.median,inplace= True)\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "def get_data(datafile):\n",
    "    df = pd.read_json(datafile)\n",
    "    df['fraud'] = df['acct_type'].apply(lambda x: 'fraud' in x)\n",
    "    y = df.pop('fraud')\n",
    "    X = df\n",
    "    return X, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = get_data('data/data.json')\n",
    "    model = MyModel()\n",
    "    model.fit(X, y)\n",
    "    with open('data/model.pkl', 'wb') as f:\n",
    "        # Write the model to a file.\n",
    "        pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict.py\n",
    "\n",
    "creates prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/predict.py\n",
    "from model import MyModel\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "X = pd.read_json('data/test_script_examples.json')\n",
    "\n",
    "with open('data/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print(model.predict_proba(X)[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_script_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.sample(10)\n",
    "test.to_json('data/test_script_examples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('data/test_script_examples.json').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Data\n",
    "\n",
    "calls API to provide realtime data<br>\n",
    "data is stored into a mongo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/client.py\n",
    "from model import MyModel\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "import boto3\n",
    "\n",
    "\n",
    "class EventAPIClient:\n",
    "    \"\"\"Realtime Events API Client\"\"\"\n",
    "\n",
    "    def __init__(self, first_sequence_number=0,\n",
    "                 api_url = 'https://hxobin8em5.execute-api.us-west-2.amazonaws.com/api/',\n",
    "                 api_key = 'vYm9mTUuspeyAWH1v-acfoTlck-tCxwTw9YfCynC',\n",
    "                 db = None):\n",
    "        \"\"\"Initialize the API client.\"\"\"\n",
    "        self.next_sequence_number = first_sequence_number\n",
    "        self.api_url = api_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "        # Create mongo instance\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client['fraud']\n",
    "        self.predictions = db['predictions']\n",
    "\n",
    "        # Create an S3 client\n",
    "        self.s3 = boto3.client('s3')\n",
    "\n",
    "        # Load model\n",
    "        with open('data/model.pkl', 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "\n",
    "    def save_to_database(self, row):\n",
    "        \"\"\"Save a data row to the database.\"\"\"\n",
    "        # Set row to pandas\n",
    "        X = pd.DataFrame([row])\n",
    "        # Predict\n",
    "        y = self.model.predict_proba(X)\n",
    "        # Append prediction\n",
    "        row['probability'] = y[0,1].round(4)\n",
    "        self.predictions.update(row, row, upsert=True)\n",
    "        print('You have {} entries in your Database'.format(self.predictions.find().count()))\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Fetch data from the API.\"\"\"\n",
    "        payload = {'api_key': self.api_key,\n",
    "                   'sequence_number': self.next_sequence_number}\n",
    "        response = requests.post(self.api_url, json=payload)\n",
    "        data = response.json()\n",
    "        self.next_sequence_number = data['_next_sequence_number']\n",
    "        return data['data']\n",
    "\n",
    "    def collect(self, interval=30):\n",
    "        \"\"\"Check for new data from the API periodically.\"\"\"\n",
    "        while True:\n",
    "            print(\"Requesting data...\")\n",
    "            data = self.get_data()\n",
    "            if data:\n",
    "                print(\"Saving...\")\n",
    "                for row in data:\n",
    "                    self.save_to_database(row)\n",
    "                ## Create csv image of database\n",
    "                df =  pd.DataFrame(list(self.predictions.find()))\n",
    "                df.to_csv('data/temp.csv', index=False)\n",
    "\n",
    "                # Uploads the given file using a managed uploader, which will split up large\n",
    "                # files automatically and upload parts in parallel.\n",
    "                self.s3.upload_file('data/temp.csv', 'dsi-fraud-casestudy', 'live.csv', ExtraArgs={'ACL': 'public-read'})\n",
    "            else:\n",
    "                print(\"No new data received.\")\n",
    "            print(f\"Waiting {interval} seconds...\")\n",
    "            time.sleep(interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load src/collect.py\n",
    "from client import EventAPIClient\n",
    "from model import MyModel\n",
    "\n",
    "# Continuously collects data\n",
    "client = EventAPIClient()\n",
    "client.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing database\n",
    "\n",
    "reads latest data from the mongo db and uploads it to the server so the Tableau dashboard can read it (tableau public does not allow for mongo connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Create a mongo client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['fraud']\n",
    "predictions = db['predictions']\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv image of database\n",
    "df =  pd.DataFrame(list(predictions.find()))\n",
    "df.to_csv('data/temp.csv', index=False)\n",
    "\n",
    "# Uploads the given file using a managed uploader, which will split up large\n",
    "# files automatically and upload parts in parallel.\n",
    "s3.upload_file('data/temp.csv', 'dsi-fraud-casestudy', 'live.csv', ExtraArgs={'ACL': 'public-read'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
